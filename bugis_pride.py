# -*- coding: utf-8 -*-
"""Bugis Pride

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D4NCuyWzvWTmjrCyfFhuFcJQtspue48b
"""

! pip install jiwer

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
from tensorflow.keras import layers, Model
import numpy as np
import librosa
import os
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import jiwer

from google.colab import drive
import tensorflow as tf
from tensorflow.keras import layers, Model
import numpy as np
import librosa
import os
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.model_selection import train_test_split
import random
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
import jiwer

# 1. Load Data
csv_file_path = '/content/drive/MyDrive/skripsi/50kalimat.csv'
audio_folder_path = '/content/drive/MyDrive/skripsi/dataAudio16Hz'

# Load the CSV and remove unnecessary columns
csv_data = pd.read_csv(csv_file_path)
csv_data = csv_data.drop(columns=['indo'])

# Menyusun data audio dan transkripsi
audio_files = []
transcriptions = []

for index, row in csv_data.iterrows():
    filename = row['filename']
    audio_file_path = os.path.join(audio_folder_path, f"{filename}.wav")
    if os.path.exists(audio_file_path):
        audio_files.append(audio_file_path)
        transcriptions.append(row['bugis'])

# Split the data into training, validation, and test sets
# 70% train, 15% validation, 15% test
X_train, X_temp, y_train, y_temp = train_test_split(audio_files, transcriptions, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# 2. Ekstraksi Fitur Audio (Mel-spectrogram)
def extract_audio_features(file_path, max_length=400):
    audio, sr = librosa.load(file_path, sr=None)
    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=80, fmax=8000)
    mel_spec = librosa.power_to_db(mel_spec, ref=np.max)

    # Padding atau memotong Mel spectrogram agar panjangnya konsisten
    if mel_spec.shape[1] < max_length:
        pad_width = max_length - mel_spec.shape[1]
        mel_spec = np.pad(mel_spec, ((0, 0), (0, pad_width)), mode='constant')
    elif mel_spec.shape[1] > max_length:
        mel_spec = mel_spec[:, :max_length]

    return mel_spec

# Ekstraksi fitur untuk data train, validation, dan test
X_train_features = [extract_audio_features(f) for f in X_train]
X_val_features = [extract_audio_features(f) for f in X_val]
X_test_features = [extract_audio_features(f) for f in X_test]

# 3. Tokenisasi Transkripsi
tokenizer = Tokenizer()
tokenizer.fit_on_texts(transcriptions)

y_train_tokenized = tokenizer.texts_to_sequences(y_train)
y_val_tokenized = tokenizer.texts_to_sequences(y_val)
y_test_tokenized = tokenizer.texts_to_sequences(y_test)

# Determine the target sequence length based on the model's output shape
target_sequence_length = X_train_features[0].shape[0]  # This is 80

# Padding untuk memastikan panjangnya seragam
y_train_tokenized = tf.keras.preprocessing.sequence.pad_sequences(y_train_tokenized, padding='post', maxlen=target_sequence_length)
y_val_tokenized = tf.keras.preprocessing.sequence.pad_sequences(y_val_tokenized, padding='post', maxlen=target_sequence_length)
y_test_tokenized = tf.keras.preprocessing.sequence.pad_sequences(y_test_tokenized, padding='post', maxlen=target_sequence_length)

# 4. Define the model
class SpeechToTextModel(Model):
    def __init__(self, vocab_size, input_shape):
        super(SpeechToTextModel, self).__init__()
        # Define the layers here without calling them
        self.lstm1 = layers.Bidirectional(layers.LSTM(128, return_sequences=True))
        self.lstm2 = layers.Bidirectional(layers.LSTM(128, return_sequences=True))
        self.dense = layers.Dense(vocab_size, activation='softmax')

    def call(self, inputs):
        # Pass the inputs through the layers in the call method
        x = self.lstm1(inputs)
        x = self.lstm2(x)
        return self.dense(x)

# Define vocab size based on your tokenizer
vocab_size = len(tokenizer.word_index) + 1  # plus one for padding

# Define the input shape (80 timesteps, 400 features)
input_shape = (80, 400)  # Example shape

# Initialize the model
model = SpeechToTextModel(vocab_size=vocab_size, input_shape=input_shape)

# Build the model with a dummy input shape to initialize its weights
model.build((None,) + input_shape)  # Add None for batch size

# Print model summary
model.summary()

# Compile model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# 5. Train the model
history = model.fit(np.array(X_train_features), np.expand_dims(y_train_tokenized, -1),
                    epochs=10, batch_size=32,
                    validation_data=(np.array(X_val_features), np.expand_dims(y_val_tokenized, -1)))

# 6. Plotting Accuracy
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.show()

# 8. Inferensi Sederhana: Prediksi Teks dari Audio
def predict_audio_to_text(model, audio_file, tokenizer):
    feature = extract_audio_features(audio_file)
    feature = np.expand_dims(feature, axis=0)  # batch dimension

    pred = model.predict(feature)
    # The model outputs a sequence of length 80, so we need to decode this sequence
    pred = np.argmax(pred, axis=-1)

    # Decode the prediction to text
    pred_text_list = []
    for seq in pred:
        decoded_seq = [tokenizer.index_word.get(idx, '') for idx in seq if idx != 0]  # Mengabaikan token padding
        pred_text_list.append(' '.join(decoded_seq))  # Gabungkan kata-kata menjadi satu kalimat

    return pred_text_list[0]

# Menentukan path file audio yang ingin diuji
audio_file_path = '/content/drive/MyDrive/skripsi/dataAudio16Hz/25_copy_1.wav'  # Ganti dengan path file audio yang sebenarnya

# Menggunakan fungsi untuk melakukan prediksi
predicted_text = predict_audio_to_text(model, audio_file_path, tokenizer)

# Menampilkan hasil prediksi
print(f"Predicted text: {predicted_text}")

# 9. Inferensi untuk 10 Audio Acak
def predict_multiple_audio_to_text(model, audio_files, actual_texts, tokenizer, num_samples=10):
    random_indices = random.sample(range(len(audio_files)), num_samples)

    predicted_texts = []
    actual_texts_subset = []

    for idx in random_indices:
        audio_file = audio_files[idx]
        actual_text = actual_texts[idx]
        predicted_text = predict_audio_to_text(model, audio_file, tokenizer)
        predicted_texts.append(predicted_text)
        actual_texts_subset.append(actual_text)

    for i in range(num_samples):
        print(f"Audio {i + 1}:")
        print(f"Predicted text: {predicted_texts[i]}")
        print(f"Actual text: {actual_texts_subset[i]}")
        print("-" * 50)
# Uji Inferensi dengan 10 audio acak dari data validasi
predict_multiple_audio_to_text(model, X_val, y_val, tokenizer, num_samples=10)

# 7. Evaluate the Model using WER
def calculate_wer(predicted, actual):
    return jiwer.wer(actual, predicted)

# Evaluate WER for test data
predictions = [predict_audio_to_text(model, file, tokenizer) for file in X_test]
wer_scores = [calculate_wer(pred, true) for pred, true in zip(predictions, y_test)]
print(f"WER: {np.mean(wer_scores)}")